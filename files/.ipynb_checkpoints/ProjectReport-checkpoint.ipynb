{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3ded32",
   "metadata": {},
   "source": [
    "# FALL 2021 IE 582 PROJECT REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881c43b",
   "metadata": {},
   "source": [
    "### Alperen KÖKSAL\n",
    "### Mehmet ÖZER\n",
    "\n",
    "### Topic: Customer Gender Prediction based on E-commerce Data\n",
    "\n",
    "#### 23.01.2022\n",
    "\n",
    "### Professor: Mustafa Gökçe BAYDOĞAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52f186",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ccd30",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc8de2",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "### The aim of the project is to predict the customer gender based on the actions taken on the online retail website. This predictions could be beneficial for different use cases such as targeted promotions. For this purpose, we are given data of the one of the biggest retail companies in Turkey. The data consist of features such as timestamp, action taken by the user, information related to the product that the page the action was taken on belongs to, unique id and gender of the action taker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05318c85",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "### There are two main approaches used to develop a solution to this business problem: \n",
    "* 1. Treating each instances as a seperate entity and build a model that maximizes the AUC on the train data. After that, predict the probabilities of each instance on the test data and grouping them by their unique ids by taking the probability averages. \n",
    "* 2. Grouping each instances by their unique ids by our proposed bag representation, then building a predictive model maximizing the predefined performance metric ( (AUC+BAR)/2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119cc27",
   "metadata": {},
   "source": [
    "## Descriptive Analysis\n",
    "\n",
    "* As the first step, data is checked to see if any duplicates exist. For train and test data, more than half of the instances are duplicates. These instances do not bring any extra information, therefore they are dropped from the data. There also exists missing values which will be dealt with later on. Except the selling price, the data consist of categorical variables. The relations between the categorical variables are investigated. Brand name is more unique than the brand id, therefore brand id is dropped. Contentid and product name have a lot of different categories. It is unlikely that they will provide any usefull information for model improvement, therefore, they are dropped as well.\n",
    "*  For all three category levels, id and name of the category represent the same thing. Thus, category level names are dropped. Comparing the category_id with Level3_Category_Id, we see that category_id is more unique than Level3_Category_Id. Grouping the data by Level3_Category_Id, each group has a unique category_id. Therefore, Level3_Category_Id is dropped. User action has five different categories. product_gender is missing in the 11% of the data. In the test data, there are unique_id's that have at least one missing value in all of their instances, therefore dropping the missing values blindly is not an option.\n",
    "\n",
    "* Product-related features have their missing values exactly at the same instances. It seems that these missing values are due to some structural reasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc15c4",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db793ee",
   "metadata": {},
   "source": [
    "### Kucuka, E.Ş., Baydoğan, M.G., “Bag Encoding Strategies in Multiple Instance Learning Problems“, Information Sciences, 467, 559-578, 2018.\n",
    "### Veronika Cheplygina, David M. J. Tax, Marco Loog, \"Multiple Instance Learning with Bag Dissimilarities\", Pattern Recognition 48.1 (2015): 264-275\n",
    "\n",
    "### These papers helped us to build a base knowledge for the bag representation of the second approach.\n",
    "\n",
    "### Tran Duc, Duong & Tan, Hanh & Pham, Son. (2016). Customer gender prediction based on E-commerce data. 91-95. 10.1109/KSE.2016.7758035. \n",
    "\n",
    "### Based on this paper, we had the idea of including the timestamp in different ways and chose our initial models as SVC and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f599504",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcedda6",
   "metadata": {},
   "source": [
    "### Throughout the project, different types of approaches are combined for the different preprocessing and model building steps. For each step, everything that we tried will be explained.\n",
    "\n",
    "* From timestamp feature, week, day, dayoftheweek, month, hour, minute and second information are extracted. Also, a feature called 'FLAG_weekend' is created that gets the value of 1 if the day is on weekend.\n",
    "\n",
    "* Before dealing with the missing values, two FLAG features are created for product_gender and selling_price features such that it gets the value of 1 if the corresponding column has missing value.\n",
    "\n",
    "* For one of the approaches, product gender is dropped due to the possibility of creating bias.\n",
    "\n",
    "## Handling the Missing Values\n",
    "\n",
    "### For missing value problem, two approaches are applied: \n",
    "* First approach is to fill the categorical variables with the most frequent values and the numerical variables with the mean. \n",
    "* Second approach is to take advantage of the Random Forest. Product related features are missing in the 0.1% of the data. This is a small proportion, therefore they are filled with the most frequent values with no problem. Afterwards, only two columns are left with missing values: selling_price and product_gender. selling_price have around 30000 values missing, far more less than the product_gender missing value count, which is 23400. Therefore, first, selling_price is imputed using other columns (except product_gender) by building a Random Forest model with selling_price as the target variable. Then, using the same logic, product_gender column is imputed using all the other columns.\n",
    "\n",
    "## Rare Encoding\n",
    "\n",
    "### Next step is to do a rare encoding. For rare encoding, two approaches are applied:\n",
    "\n",
    "* In the first approach, train and test data are merged. By analysing the levels of all the categorical features, percentage thresholds which balances the dimension and the information loss is defined. Then, rare encoding is done such that, for all categories, levels of that feature that has frequency less than the threshold defined for that feature are transformed into a new level called 'Rare'. One hot encoding is applied to the categorical features. Then, train and test data are split.\n",
    "\n",
    "* In the second approach, levels that are not common in the train and test data are converted into a new level called 'Other'. After that, two data are merged and rare encoding is applied such that levels that have frequency less then 1% are transformed into a new level called 'Rare'. Then, train and test data are split. Label encoding is applied to each categoric feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43785c79",
   "metadata": {},
   "source": [
    "## Extra Steps\n",
    "\n",
    "* One of the approaches was to apply PCA to reduce the dimensionality of the data while keeping as much of the variance as possible.\n",
    "\n",
    "* Another approach was to apply undersampling and oversampling (SMOTE) to deal with class imbalances. Data consist of more female-labeled instances and it is possible that model favors towards the frequent class. Undersampling is done by selecting from female population with replacement such that #females = #males. SMOTE is done by creating new male instances by using the k_neighbors algorithm on the existing male instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1e8f5",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### In general, for model building, train data is split into two datasets: Train and Validation. Model is trained on the train data with cross validation and validated on the validation data.\n",
    "* When SVC is applied for the data, all the test instances are labeled as Female. It is decided that SVC is not a good fit for this problem.\n",
    "\n",
    "* Random Forest had good results. It is robust to some features dominating the overall results and it has low variance due to the high number of estimators. Robustness to the feature dominance was a good fit for this problem because product_gender had a potential to dominate the results and could create a bias. These are the reasons for us to choose Random Forest as one of our main algorithms.\n",
    "\n",
    "* LightGBM and CatBoost are similar algorithms that we used. Both of them are good at dealing with high cardinality categorical features (CatBoost is probably superior in this manner) and they have in-built ways to handle missing data (This was useful in the first phase of the project that we didn't decide the impute the missing values yet). CatBoost was the first choice that was focused on, however, it has an isolated structure that is not compatible with sklearn and it was not possible to use some good features of the other libraries. For these reasons, CatBoost is abondoned.\n",
    "\n",
    "* LightGBM was fast and compatible with other libraries. For these reasons, we kept using it together with the other algorithms.\n",
    "\n",
    "* The last model building approach we used was AutoML of the H2O package. It is an automated ML library that does the parameter tuning and ensembling steps automatically, given the proper dataset. For AutoML, dataset must be converted to H2O Frame and variable types must be defined. Data is given, stopping criteria is selected (# of models to try or number of seconds) and it is started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49eca0",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac9b9c",
   "metadata": {},
   "source": [
    "### Images represent the training metrics. Validation score is obtained from the validation set that is seperated before training. Submission score is score obtained from the submission of the test data. For AutoML models, the best model is used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a20b86",
   "metadata": {},
   "source": [
    "### RF (Submission AUC= 0.845, Submission Performance= 0.804)\n",
    "![title](images/rf_raw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7abd79",
   "metadata": {},
   "source": [
    "### RF with PCA\n",
    "![title](images/rf_pca.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29105586",
   "metadata": {},
   "source": [
    "### SVC\n",
    "![title](images/svc_raw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b3cbc",
   "metadata": {},
   "source": [
    "### SVC with PCA\n",
    "![title](images/svc_pca.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8884c1",
   "metadata": {},
   "source": [
    "### AutoML CV  (10 mins) (Validation AUC= 0.87, Submission AUC= 0.839, Submission Performance= 0.707)\n",
    "![title](images/automl_600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11787a13",
   "metadata": {},
   "source": [
    "### AutoML CV  (10 mins) (Validation AUC= 0.90, Submission AUC= 0.833, Submission Performance= 0.805)\n",
    "![title](images/automl_4800.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f8128",
   "metadata": {},
   "source": [
    "### AutoML Feature Importances\n",
    "![title](images/importance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf7d33",
   "metadata": {},
   "source": [
    "### AutoML Feature Importances (without product_gender)\n",
    "![title](images/importance_without_gender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875adb",
   "metadata": {},
   "source": [
    "# Conclusions and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1626c4",
   "metadata": {},
   "source": [
    "### As the final result, we got a performance ((BAR+AUC)/2) value of ~0.80. It has an AUC score of 0.845 and BAR score of 0.76. It is a good AUC score overall. On the validation set, the model got an AUC score of ~0.91. Considering that data was split carefully to prevent any leakage, drop of 0.065 in the AUC score is not something to be expected. Even though data size was large enough to prevent overfitting, it seems like there were some overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b336d4",
   "metadata": {},
   "source": [
    "### Clearly, there is scope for future improvement in this project. For the group by approach, features such as 'percentage of the actions taken on the specific category level', 'percentage of the actions taken in the specific time interval', 'percentage of the action taken in the specific price interval' could be added. Some useful information could be extracted from the product name. Features that are similar to those used in RFM applications could be added, such as 'Frequency of the purchases', 'Monetary value of the purchases', 'RFM values of the person', interaction between selling price and action types etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11071f7",
   "metadata": {},
   "source": [
    "# Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81da99",
   "metadata": {},
   "source": [
    "## Group By Approach: https://github.com/BU-IE-582/fall21-sencer4898/blob/gh-pages/files/project_codes/groupby_source_code.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2058854",
   "metadata": {},
   "source": [
    "## Individual Instances Approach: https://github.com/BU-IE-582/fall21-sencer4898/blob/gh-pages/files/project_codes/individual_treatment_source_code.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad5fe86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line is used to create html version of the notebook.\n",
    "\n",
    "import os\n",
    "\n",
    "os.system('jupyter nbconvert --to html ProjectReport.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
